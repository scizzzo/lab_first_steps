{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 90\n",
    "\n",
    "data_folders = os.listdir('./letters/')\n",
    "\n",
    "data_folders = [os.path.join('./letters/', d) for d in data_folders if os.path.isdir(os.path.join('./letters/', d))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_letter(folder):\n",
    "    files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape = (len(files), image_size, image_size), dtype=np.float32)\n",
    "    img_num = 0\n",
    "    for image in files:\n",
    "        if 'DS' not in image:\n",
    "            image_name = os.path.join(folder, image)\n",
    "            image_data = ndimage.imread(image_name, mode = 'L')\n",
    "            dataset[img_num] = image_data\n",
    "            img_num += 1\n",
    "    return dataset\n",
    "def image_pickling(folders):\n",
    "    dataset_names = []\n",
    "    for folder in folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        dataset = load_letter(folder)\n",
    "        try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Something is wrong', set_filename, ':', e )\n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = image_pickling(data_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_datasets(pickle_files, set_size):\n",
    "    num_classes = len(pickle_files)\n",
    "    dataset = np.ndarray(shape = (set_size * num_classes, image_size, image_size), dtype = np.float32)\n",
    "    labels = np.ndarray(set_size * num_classes, dtype = np.int32)\n",
    "    start_pos = 0\n",
    "    for label, pickle_file in enumerate(sorted(pickle_files)):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                np.random.shuffle(letter_set)\n",
    "                dataset[start_pos:start_pos + set_size, :, :] = letter_set[:set_size, :, :]\n",
    "                labels[start_pos:start_pos + set_size] = label\n",
    "                start_pos += set_size\n",
    "        except Exception as e:\n",
    "            print('Something is wrong', pickle_file, ':', e)\n",
    "    dataset /= 255\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset, labels = merge_datasets(datasets, set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutaion = np.random.permutation(dataset.shape[0])\n",
    "    dataset = dataset[permutaion]\n",
    "    labels = labels[permutaion]\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset, labels = randomize(dataset, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 4\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset, labels, random_state = 241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3000, 90, 90, 1)\n",
      "3000 train samples\n",
      "1000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0],  image_size, image_size, 1)\n",
    "input_shape = (image_size, image_size, 1)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 1000 samples\n",
      "Epoch 1/12\n",
      "3000/3000 [==============================] - 186s - loss: 1.6956 - acc: 0.2683 - val_loss: 1.3675 - val_acc: 0.3520\n",
      "Epoch 2/12\n",
      "3000/3000 [==============================] - 173s - loss: 1.3647 - acc: 0.2993 - val_loss: 1.3566 - val_acc: 0.4570\n",
      "Epoch 3/12\n",
      "3000/3000 [==============================] - 179s - loss: 1.3397 - acc: 0.3453 - val_loss: 1.3100 - val_acc: 0.4470\n",
      "Epoch 4/12\n",
      "3000/3000 [==============================] - 181s - loss: 1.3084 - acc: 0.3607 - val_loss: 1.2412 - val_acc: 0.3840\n",
      "Epoch 5/12\n",
      "3000/3000 [==============================] - 23182s - loss: 1.2323 - acc: 0.4030 - val_loss: 1.0704 - val_acc: 0.6290\n",
      "Epoch 6/12\n",
      "3000/3000 [==============================] - 170s - loss: 1.0970 - acc: 0.4747 - val_loss: 0.8771 - val_acc: 0.7920\n",
      "Epoch 7/12\n",
      "3000/3000 [==============================] - 182s - loss: 1.0103 - acc: 0.5273 - val_loss: 0.7806 - val_acc: 0.6390\n",
      "Epoch 8/12\n",
      "3000/3000 [==============================] - 174s - loss: 0.8479 - acc: 0.6313 - val_loss: 0.4889 - val_acc: 0.8620\n",
      "Epoch 9/12\n",
      "3000/3000 [==============================] - 173s - loss: 0.6535 - acc: 0.7137 - val_loss: 0.3151 - val_acc: 0.9110\n",
      "Epoch 10/12\n",
      "3000/3000 [==============================] - 172s - loss: 0.6024 - acc: 0.7357 - val_loss: 0.2005 - val_acc: 0.9610\n",
      "Epoch 11/12\n",
      "3000/3000 [==============================] - 173s - loss: 0.4817 - acc: 0.7840 - val_loss: 0.1481 - val_acc: 0.9630\n",
      "Epoch 12/12\n",
      "3000/3000 [==============================] - 173s - loss: 0.4556 - acc: 0.7960 - val_loss: 0.1286 - val_acc: 0.9730\n",
      "Test loss: 0.128609986186\n",
      "Test accuracy: 0.973\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "3000/3000 [==============================] - 175s - loss: 0.3787 - acc: 0.8337 - val_loss: 0.1542 - val_acc: 0.9500\n",
      "Epoch 2/3\n",
      "3000/3000 [==============================] - 173s - loss: 0.3513 - acc: 0.8503 - val_loss: 0.0808 - val_acc: 0.9920\n",
      "Epoch 3/3\n",
      "3000/3000 [==============================] - 174s - loss: 0.3139 - acc: 0.8603 - val_loss: 0.0628 - val_acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1333e52e8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
